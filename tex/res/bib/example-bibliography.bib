
@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-10-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017}
}


@article{mermin_whats_1989,
	title = {What's {Wrong} with these {Equations}?},
	volume = {42},
	issn = {0031-9228},
	url = {https://bauwenst.github.io/cdn/doc/pdf/2024/Mermin-1989_Whats-Wrong-With-These-Equations.pdf},
	doi = {10.1063/1.2811173},
	number = {10},
	urldate = {2024-01-07},
	journal = {Physics Today},
	author = {Mermin, N. David},
	month = oct,
	year = {1989},
	pages = {9--11}
}


@article{kettles_law_2023,
	title = {A {Law} {Firm} {Said} {Plagiarism} {Allegations} {Against} {Harvard} {President} {Gay} {Were} ‘{Demonstrably} {False}.’ {Then} {She} {Submitted} {Corrections}.},
	url = {https://www.thecrimson.com/article/2023/12/25/harvard-threaten-sue-post/},
	abstract = {Harvard threatened to sue the New York Post for defamation over accusations of plagiarism against President Claudine Gay in October, calling the claims “demonstrably false.” Then, the University’s own review found several instances of “duplicative language” in Gay’s work.},
	urldate = {2024-01-07},
	journal = {The Harvard Crimson},
	author = {Kettles, Cam E. and Robinson, Tilly R.},
	month = dec,
	year = {2023},
}

@inproceedings{bauwens_bpe-knockout_2023,
	title = {{BPE}-knockout: {Systematic} review of {BPE} tokenisers and their flaws with application in {Dutch} morphology},
	booktitle = {{BPE-knockout}},
	url = {https://bauwenst.github.io/cdn/doc/pdf/2023/masterthesis.pdf},
	publisher = {Master's thesis, KU Leuven},
	author = {Bauwens, Thomas},
	year = {2023},
}

@inproceedings{bauwens_bpe-knockout_2024,
	address = {Mexico City, Mexico},
	title = {{BPE}-knockout: {Pruning} {Pre}-existing {BPE} {Tokenisers} with {Backwards}-compatible {Morphological} {Semi}-supervision},
	shorttitle = {{BPE}-knockout},
	url = {https://aclanthology.org/2024.naacl-long.324},
	doi = {10.18653/v1/2024.naacl-long.324},
	abstract = {Byte-pair encoding (BPE) has become the default subword tokeniser in language models (LMs), allowing the representation of an infinite space of text with a finite set of units. Yet, BPE training is unsupervised, receiving no explicit information about a language's morphology. This results in a subword vocabulary wherein many units are a concatenation of partial morphemes, preventing their formation as tokens. This, in turn, causes consistent intra-word patterns to be displayed inconsistently to downstream models, and bloats the vocabulary, hence requiring unnecessary embedding storage. In this paper, we address this issue by identifying blameworthy BPE merges and removing the resulting subwords from the BPE vocabulary, without impeding further use of merges that relied on them. We find that our method, BPE-knockout, is effective at making BPE's segmentation positions adhere better to derivational and compound boundaries in English, Dutch and German, and improves token-based tasks in Dutch RoBERTa models, indicating that a tokeniser's adherence to morphology impacts downstream models. We demonstrate the latter not only by training LMs from scratch, but also by continuing the pre-training of existing LMs. This proves promising, showing that suboptimal tokenisers can be remedied whilst salvaging training cost of downstream LMs.},
	urldate = {2024-09-28},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bauwens, Thomas and Delobelle, Pieter},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {5810--5832}
}

@article{hardy1917asymptotic,
  title={Asymptotic {Formul\ae} for the {Distribution of Integers of Various Types}},
  author={Hardy, Godfrey Harold and Ramanujan, Srinivasa},
  journal={Proceedings of the London Mathematical Society},
  volume={2},
  number={1},
  pages={112--132},
  year={1917},
  publisher={Oxford University Press},
  url={https://ia600708.us.archive.org/view_archive.php?archive=/28/items/crossref-pre-1923-scholarly-works/10.1112%252Fplms%252Fs2-10.1.116.zip&file=10.1112%252Fplms%252Fs2-16.1.112.pdf}
}